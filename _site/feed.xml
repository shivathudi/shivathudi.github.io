<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-08-03T17:03:52-07:00</updated><id>http://localhost:4000/</id><title type="html">Shiva Thudi</title><subtitle></subtitle><entry><title type="html">Gradient Boosted Trees</title><link href="http://localhost:4000/jekyll/update/2017/08/03/gbt-writeup.html" rel="alternate" type="text/html" title="Gradient Boosted Trees" /><published>2017-08-03T10:00:00-07:00</published><updated>2017-08-03T10:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2017/08/03/gbt-writeup</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/08/03/gbt-writeup.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;In this post I will explore how Gradient Boosted Trees work. Gradient Boosting is a very popular and widely used (“&lt;em&gt;off-the-shelf&lt;/em&gt;”) machine learning algorithm that often surpasses other algorithms in terms of predictive power. In almost every project and data challenge I have done at work and school, I have ended up using gradient boosting as my final model.&lt;/p&gt;

&lt;p&gt;First I will start by explaining what boosting is and how it came into practice. I will then explore how gradient boosted trees work. I will use pseudocode along the way, and I will be referencing illustrations and notes from &lt;a href=&quot;http://web.stanford.edu/~hastie/ElemStatLearn/&quot;&gt;The Elements of Statistical Learning&lt;/a&gt; (ESL) by Friedman, Tibshirani, and Hastie. Initially, I will talk about boosting and loss functions from a binary classification perspective, but it can be extended to multi-class classification and regression problems as well.&lt;/p&gt;

&lt;h2 id=&quot;boosting---introduction-and-adaboost&quot;&gt;Boosting - Introduction and AdaBoost&lt;/h2&gt;

&lt;p&gt;Boosting is a procedure that combines the outputs of many weak classifiers (usually decision trees) to produce a powerful “committee”. It is similar to other committee-based approaches like bagging. In bagging, we take an average of the predictions made by building models on bootstrapped (sampling with replacement) datasets. Since each model is trained on a different bootstrapped dataset, this approach can be parallelized.&lt;/p&gt;

&lt;p&gt;Boosting, however, is a committee-based, sequential process that can’t be parallelized and which doesn’t use bootstrapping. AdaBoost.M1, a popular boosting algorithm due to Freund and Schapire (1997), works by training models on successive versions of the original data where observations that were misclassified by the previous model in the sequence are given higher weights than those that were correctly classified. Each model/tree in the sequence tries to correct for the errors made by the previous model. After a chosen number of modeling iterations, the final prediction is a weighted majority vote where each classifier is weighted according to its accuracy on the data it was trained on.&lt;/p&gt;

&lt;h3 id=&quot;adaboost-algorithm-from-esl-friedman-et-al&quot;&gt;AdaBoost algorithm from ESL, Friedman et al:&lt;/h3&gt;

&lt;p&gt;We assume that the output variable is coded as Y ∈ {−1, 1}. This is the AdaBoost algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/AdaBoost.M1.png&quot; alt=&quot;illust1&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In practice, the classifiers or weak learners used in AdaBoost are &lt;strong&gt;stumps&lt;/strong&gt;, which are decision trees with a single split.&lt;/p&gt;

&lt;p&gt;In ESL, Friedman et al. show how AdaBoost.M1 is equivalent to forward-stagewise additive modeling using the exponential loss function.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Minimizing loss of additive model&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/additive.png&quot; alt=&quot;illust2&quot; height=&quot;75px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exponential loss&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(y, f (x)) = e^{(−y f (x))}&lt;/script&gt;

&lt;p&gt;They also state why a stagewise rather than a stepwise approach is used:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For many loss functions &lt;script type=&quot;math/tex&quot;&gt;L(y,f(x))&lt;/script&gt; and/or basis functions &lt;script type=&quot;math/tex&quot;&gt;b(x;γ)&lt;/script&gt;, this approach (stepwise) requires computationally intensive numerical optimization techniques. However, a simple alternative often can be found when it is feasible to rapidly solve the subproblem of fitting just a single basis function. Forward stagewise modeling approximates the solution by sequentially adding new basis functions to the expansion without adjusting the parameters and coefficients of those that have already been added.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hence, we can think of boosting as a numerical optimization problem where we have to minimize the loss using stagewise procedures. One weak learner is added at a time, without changing the previously computed parameters and coefficients of earlier models in the sequence.&lt;/p&gt;

&lt;h2 id=&quot;gradient-boosted-trees&quot;&gt;Gradient Boosted Trees&lt;/h2&gt;

&lt;p&gt;Gradient Boosted Trees are an ensemble of decision trees that are built in a stagewise fashion, allowing optimization of &lt;strong&gt;any differentiable loss function&lt;/strong&gt;. In fact, for binary classification problems using the exponential loss function, Gradient Boosting is equivalent to AdaBoost.&lt;/p&gt;

&lt;p&gt;When a tree is added one at a time in the stagewise approach, we use a gradient descent procedure to minimize the loss. We do this by parametrizing the trees and moving in the direction indicated by gradient descent, reducing the residual loss at that step. This approach is referred to as functional gradient descent.&lt;/p&gt;

&lt;h3 id=&quot;tree-parameters-and-approach&quot;&gt;Tree parameters and approach&lt;/h3&gt;

&lt;p&gt;Decision trees work by partitioning the feature space into disjoint regions &lt;script type=&quot;math/tex&quot;&gt;R_{j} , j . . . J&lt;/script&gt;. For x in particular region &lt;script type=&quot;math/tex&quot;&gt;R_j&lt;/script&gt; the value of T(x) is constant. A tree can be formally expressed as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(x,\gamma) =  \sum_{j=1}^{J} \beta_{j} I(x ∈ R_j)&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;I(x ∈ R_j)&lt;/script&gt; is an indicator variable that takes the value of 1 if x belongs to region &lt;script type=&quot;math/tex&quot;&gt;R_j&lt;/script&gt;, or 0 otherwise.&lt;/p&gt;

&lt;p&gt;The boosted tree model is a sum of M trees&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{M} (x) = \sum_{m=1}^{M} T(x,\gamma_{m})&lt;/script&gt;

&lt;p&gt;induced in a forward stagewise manner. At each step &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;, we first find the regions &lt;script type=&quot;math/tex&quot;&gt;\{ R_{jm} \}_{m=1}^J&lt;/script&gt; by solving&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\gamma_m = arg min_\gamma \sum_{i=1}^{N} L(y_i,f_{m−1}(x_i)+ T(x_i,\gamma))&lt;/script&gt;

&lt;p&gt;Given the regions &lt;script type=&quot;math/tex&quot;&gt;R_{jm}&lt;/script&gt;,  finding the optimal constants for each region can be done by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;β_{jm} = arg min_\beta \sum_{x_{i} ∈ R_{jm}}^{} L(y_i, f_{m−1}(x_i) + β))&lt;/script&gt;

&lt;h3 id=&quot;gradient-boosted-trees-for-regression-with-squared-loss&quot;&gt;Gradient Boosted Trees for Regression with Squared Loss&lt;/h3&gt;

&lt;p&gt;To make the problem more clear, let’s consider the case of the squared loss for regression.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/gbt_regr.png&quot; alt=&quot;illust3&quot; height=&quot;175px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;r_{im} = y_{i} − f_{m−1} (x_i)&lt;/script&gt; is the residual for observation &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt; at iteration &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt;. This problem is a standard regression tree problem that can be solved using CART or any other regression tree method.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;β_{jm} = arg min_\beta \sum_{x_{i} ∈ R_{jm}}^{} L(y_i, f_{m−1}(x_i) + β)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= arg min_\beta \sum_{x_{i} ∈ R_{jm}}^{} (r_{im}-β)^2&lt;/script&gt;

&lt;p&gt;This last problem has an analytical solution where &lt;script type=&quot;math/tex&quot;&gt;β&lt;/script&gt; is the average of &lt;script type=&quot;math/tex&quot;&gt;r_{im}&lt;/script&gt; over &lt;script type=&quot;math/tex&quot;&gt;R_{jm}&lt;/script&gt;.&lt;/p&gt;

&lt;h3 id=&quot;algorithm-and-other-loss-functions-illustrations-taken-from-esl&quot;&gt;Algorithm and other loss functions (illustrations taken from ESL)&lt;/h3&gt;

&lt;p&gt;We have seen how gradient boosted trees work for regression using a squared loss. We can also use different loss criteria; the following illustration outlines the generic gradient tree-boosting algorithm for regression:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/gbt-alg.png&quot; alt=&quot;illust4&quot; height=&quot;420px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Specific algorithms are obtained by inserting different loss criteria &lt;script type=&quot;math/tex&quot;&gt;L(y,f(x))&lt;/script&gt;.  Gradients for commonly used loss functions are shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/gbt-loss.png&quot; alt=&quot;illust4&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;tree-constraints-and-regularization&quot;&gt;Tree constraints and regularization&lt;/h4&gt;

&lt;p&gt;There are certain tree constraints that we can choose to tune:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number of trees, M - The main tuning parameter, it is often picked by monitoring the performance on a separate validation set, and then stopping once performance starts to decrease.&lt;/li&gt;
  &lt;li&gt;Tree depth - Deeper trees capture more interactions and complexities, usually a depth between 4 and 8 suffices.&lt;/li&gt;
  &lt;li&gt;Number of nodes&lt;/li&gt;
  &lt;li&gt;Minimum loss reduction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can also use L1 or L2 regularization terms to avoid overfitting.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Matrix multiplication using the Divide and Conquer paradigm</title><link href="http://localhost:4000/jekyll/update/2017/06/15/matr-mult.html" rel="alternate" type="text/html" title="Matrix multiplication using the Divide and Conquer paradigm" /><published>2017-06-15T20:43:02-07:00</published><updated>2017-06-15T20:43:02-07:00</updated><id>http://localhost:4000/jekyll/update/2017/06/15/matr-mult</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/06/15/matr-mult.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;In this post I will explore how the divide and conquer algorithm approach is applied to matrix multiplication. I will start with a brief introduction about how matrix multiplication is generally observed and implemented, apply different algorithms (such as Naive and Strassen) that are used in practice with both pseduocode and Python code, and then end with an analysis of their runtime complexities. There will be illustrations along the way, which are taken from Cormen’s textbook &lt;em&gt;Introduction to Algorithms&lt;/em&gt; and Tim Roughgarden’s slides from his &lt;em&gt;Algorithms specialization on Coursera&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction-and-motivation&quot;&gt;Introduction and Motivation&lt;/h2&gt;

&lt;p&gt;Matrix multiplication is a fundamental problem in computing. Tim Roughgarden states that&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Computers as long as they’ve been in use from the time they were invented uptil today, a lot of their cycles is spent multiplying matrices. It just comes up all the time in important applications.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;From Wikipedia:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The definition of matrix multiplication is motivated by linear equations and linear transformations on vectors, which have numerous applications in applied mathematics, physics, and engineering.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Since it is such a central operation in many applications, matrix multiplication is one of the most well-studied problems in numerical computing. Various algorithms have been devised for computing the matrix product, especially for large matrices.&lt;/p&gt;

&lt;h3 id=&quot;asymptotic-notation&quot;&gt;Asymptotic notation&lt;/h3&gt;

&lt;p&gt;Before we start, let us briefly talk about asymptotic notation. Asymptotic notation is primarily used to describe the running times of algorithms, with each type of notation specifying a different bound (lower, upper, tight bound, etc.).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Big-Theta: &lt;script type=&quot;math/tex&quot;&gt;\Theta&lt;/script&gt;-notation bounds a function to within constant factors.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Big-Oh: &lt;script type=&quot;math/tex&quot;&gt;O&lt;/script&gt;-notation gives an upper bound for a function to within a constant factor.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Big-Omega: &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt;-notation gives a lower bound for a function to within a constant factor.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/big.png&quot; alt=&quot;illust100&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Small o-notation is used to denote an upper bound that is not asymptotically tight. The definitions of O-notation and o-notation are similar, but the main difference is that in &lt;script type=&quot;math/tex&quot;&gt;f(n) = O(g(n))&lt;/script&gt;, the bound &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0 &lt;= f(n) &lt;= c(g(n)) %]]&gt;&lt;/script&gt; holds for some constant c &amp;gt; 0, but in &lt;script type=&quot;math/tex&quot;&gt;f(n) = o(g(n))&lt;/script&gt;, the bound &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
0 &lt;= f(n) &lt; c(g(n)) %]]&gt;&lt;/script&gt; holds for all constants c &amp;gt; 0. By analogy, &lt;script type=&quot;math/tex&quot;&gt;\omega&lt;/script&gt;-notation (small omega) is to  &lt;script type=&quot;math/tex&quot;&gt;\Omega&lt;/script&gt;-notation (big omega) as o-notation is to O-notation. Cormen draws an analogy between the asymptotic comparison of two functions f and g and the comparison of two real numbers a and b as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/analogy.png&quot; alt=&quot;illust100&quot; height=&quot;150px&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;naive-matrix-multiplication&quot;&gt;Naive matrix multiplication&lt;/h3&gt;

&lt;p&gt;Let us start with two square matrices A and B which are both of size n by n. In the product C = A X B we define the entry &lt;script type=&quot;math/tex&quot;&gt;c_{ij}&lt;/script&gt;,  the entry in the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; row and the &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; column of A,  as being the dot product of the &lt;script type=&quot;math/tex&quot;&gt;i^{th}&lt;/script&gt; row of A with the &lt;script type=&quot;math/tex&quot;&gt;j^{th}&lt;/script&gt; column of B. Taking the dot product of vectors just means that you take the products of the individual components and then add up the results.&lt;/p&gt;

&lt;p&gt;The following illustrations, taken from Introduction to Algortihms, Cormen et. al, sum this up nicely:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/illust_1.png&quot; alt=&quot;illust1&quot; height=&quot;75px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/square-multiply.png&quot; alt=&quot;illust2&quot; title=&quot;Natural/Naive method&quot; height=&quot;250px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first for loop at line 3 computes the entries of each row i, and within a given row i, the for loop from line 4 computes each of the entries &lt;script type=&quot;math/tex&quot;&gt;c_{ij}&lt;/script&gt; for each column j. Line 5 computes each entry &lt;script type=&quot;math/tex&quot;&gt;c_{ij}&lt;/script&gt; using the dot product, which was defined in the equation above. We can translate the pseudocode to Python code as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
n,n = np.shape(A)
C = np.eye(n)
for i in range(n):
	for j in range(n):
		C[i][j] = 0
		for k in range (n):
			C[i][j] += A[i][k]*B[k][j]
return C
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Each of the triply-nested for loops above runs for exactly n iterations, which means that the algorithm above has a runtime complexity of &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^3)&lt;/script&gt;. Roughgarden poses the following question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;So the question as always for the keen algorithm designer is, can we do better? Can we beat &lt;script type=&quot;math/tex&quot;&gt;n^3&lt;/script&gt; time for multiplying two matrices?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cormen et al. note that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You might at first think that any matrix multiplication algorithm must take &lt;script type=&quot;math/tex&quot;&gt;\omega(n^3)&lt;/script&gt; time, since the natural definition of matrix multiplication requires that many multiplications. You would be incorrect, however: we have a way to multiply matrices in  &lt;script type=&quot;math/tex&quot;&gt;o(n^3)&lt;/script&gt; time. Strassen’s remarkable recursive algorithm for multiplying n by n matrices runs in &lt;script type=&quot;math/tex&quot;&gt;\Theta (n^{lg7})&lt;/script&gt; time.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So how is Strassen’s algorithm better? To answer that, we will first look at how we can apply the divide and conquer approach for multiplying matrices.&lt;/p&gt;

&lt;h2 id=&quot;divide-and-conquer-using-block-partitioning&quot;&gt;Divide and Conquer using Block Partitioning&lt;/h2&gt;

&lt;p&gt;Let us first assume that n is an exact power of 2 in each of the n x n matrices for A and B. This simplyifying assumption allows us to break a big n x n matrix into smaller blocks or quadrants of size n/2 x n/2, while also ensuring that the dimension n/2 is an integer. This process is termed as &lt;strong&gt;block partitioning&lt;/strong&gt; and the good part about it is that once matrices are split into blocks and multiplied, the blocks behave as if they were atomic elements. The product of A and B can then be expressed in terms of its quadrants. This gives us an idea of a recursive approach where we can keep partitioning matrices into quadrants until they are small enough to be multiplied in the naive way.&lt;/p&gt;

&lt;p&gt;The following equations from Cormen et al. illustrate this process:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/cormen_recur.png&quot; alt=&quot;cormen1&quot; title=&quot;Block Partitioning&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In each of the 4 equations above we have two multiplications of n/2 x n/2 matrices that are then added together. A recursive, divide-and-conquer algorithm is then:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/cormen_recur2.png&quot; alt=&quot;cormen2&quot; title=&quot;Divide and Conquer with Block Partitioning&quot; height=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For multiplying two matrices of size n x n, we make 8 recursive calls above, each on a matrix/subproblem with size n/2 x n/2. Each of these recursive calls multiplies two n/2 x n/2 matrices, which are then added together. For the addition, we add two matrices of size &lt;script type=&quot;math/tex&quot;&gt;n^2/4&lt;/script&gt;, so each addition takes &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2/4)&lt;/script&gt; time. We can write this recurrence in the form of the following equations (taken from Cormet et al.):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/recurrence.png&quot; alt=&quot;recurrence&quot; title=&quot;Recurrence with Block Partitioning&quot; height=&quot;60px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The master theorem is a way of figuring out the runtime complexity of algorithms that use the divide and conquer approach, where subproblems are of equal size. It was popularized by Cormen et al. Roughgarden refers to it as a &lt;em&gt;“black box for solving recurrences”&lt;/em&gt;. The following equations are taken from his slides:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/master1.png&quot; alt=&quot;master1&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/master2.png&quot; alt=&quot;master2&quot; height=&quot;200px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For our block partitioning approach, we saw that we had 8 recursive calls &lt;script type=&quot;math/tex&quot;&gt;(a=8)&lt;/script&gt;, where each subproblem was of size n/2 x n/2 &lt;script type=&quot;math/tex&quot;&gt;(b=2)&lt;/script&gt;. Outside of the recursive calls, we were performing additions that were of the order &lt;script type=&quot;math/tex&quot;&gt;n^2/4&lt;/script&gt; since each quadrant matrix had those many entries. So we were doing work of the order of &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2)&lt;/script&gt; outside of the recursive calls (&lt;script type=&quot;math/tex&quot;&gt;d=2&lt;/script&gt;). This corresponds to case 3 of the master theorem since &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; (8) is more than &lt;script type=&quot;math/tex&quot;&gt;b^d&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;2^2 = 4&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;Using the master theorem, we can say that the runtime complexity is big &lt;script type=&quot;math/tex&quot;&gt;O(n^{log_b a})&lt;/script&gt;, which is big &lt;script type=&quot;math/tex&quot;&gt;O(n^{log_2 8})&lt;/script&gt;or big &lt;script type=&quot;math/tex&quot;&gt;O(n^3)&lt;/script&gt;. This is no better than the straightforward iterative algorithm!&lt;/p&gt;

&lt;h2 id=&quot;strassens-algorithm&quot;&gt;Strassen’s Algorithm&lt;/h2&gt;

&lt;p&gt;Strassen’s algorithm makes use of the same divide and conquer approach as above, but instead uses only 7 recursive calls rather than 8. This is enough to reduce the runtime complexity to sub-cubic time! See the following quote from Cormen:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The key to Strassen’s method is to make the recursion tree slightly less bushy. Strassen’s method is not at all obvious. (This might be the biggest understatement in this book.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We save one recursive call, but have several new additions of n/2 x n/2 matrices. Strassen’s algorithm has four steps:&lt;/p&gt;

&lt;p&gt;1)  Divide the input matrices A and B into &lt;script type=&quot;math/tex&quot;&gt;n/2&lt;/script&gt; x &lt;script type=&quot;math/tex&quot;&gt;n/2&lt;/script&gt; submatrices, which takes &lt;script type=&quot;math/tex&quot;&gt;\Theta(1)&lt;/script&gt; time by performing index calculations.&lt;/p&gt;

&lt;p&gt;2) Create 10 matrices &lt;script type=&quot;math/tex&quot;&gt;S_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;S_2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;S_3&lt;/script&gt;, … &lt;script type=&quot;math/tex&quot;&gt;S_{10}&lt;/script&gt; each of which is the sum or difference of two matrices created in step 1. We can create all 10 matrices in &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2)&lt;/script&gt; time.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;http://localhost:4000/assets/strassen_sumatrices_10.png&quot; alt=&quot;strassen_sumatrices_10&quot; height=&quot;200px&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;3) Using the submatrices created from both of the steps above, recursively compute seven matrix products &lt;script type=&quot;math/tex&quot;&gt;P_1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;P_2&lt;/script&gt;, … &lt;script type=&quot;math/tex&quot;&gt;P_7&lt;/script&gt;. Each matrix &lt;script type=&quot;math/tex&quot;&gt;P_{i}&lt;/script&gt; is of size n/2 x n/2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/strassen_recur_7.png&quot; alt=&quot;strassen_recur_7&quot; height=&quot;150px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The products that we compute in our recursive calls are the ones from the middle column above. The right column just shows what these products equal in terms of the original submatrices from step 1.&lt;/p&gt;

&lt;p&gt;4) Get the desired submatrices &lt;script type=&quot;math/tex&quot;&gt;C_{11}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;C_{12}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;C_{21}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;C_{22}&lt;/script&gt; of the result matrix C by adding and subtracting various combinations of the &lt;script type=&quot;math/tex&quot;&gt;P_i&lt;/script&gt; submatrices. These four submatrices can be computed in &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2)&lt;/script&gt; time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/C1.png&quot; alt=&quot;&quot; title=&quot;Recurrence with Block Partitioning&quot; height=&quot;40px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/C2.png&quot; alt=&quot;&quot; title=&quot;Recurrence with Block Partitioning&quot; height=&quot;35px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/C3.png&quot; alt=&quot;&quot; title=&quot;Recurrence with Block Partitioning&quot; height=&quot;35px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/C4.png&quot; alt=&quot;&quot; title=&quot;Recurrence with Block Partitioning&quot; height=&quot;30px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using the above steps, we get the recurrence of the following format:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/recurrence2.png&quot; alt=&quot;recurrence2&quot; title=&quot;Recurrence with Block Partitioning&quot; height=&quot;60px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In Strassen’s algorithm, we saw that we had 7 recursive calls &lt;script type=&quot;math/tex&quot;&gt;(a=7)&lt;/script&gt;, where each subproblem or submatrix was of size n/2 x n/2 &lt;script type=&quot;math/tex&quot;&gt;(b=2)&lt;/script&gt;. Outside of the recursive calls, we were performing work on the order of &lt;script type=&quot;math/tex&quot;&gt;\Theta(n^2)&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;d=2&lt;/script&gt;). This also corresponds to case 3 of the master theorem since &lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt; (7) is more than &lt;script type=&quot;math/tex&quot;&gt;b^d&lt;/script&gt; (&lt;script type=&quot;math/tex&quot;&gt;2^2 = 4&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;However, unlike the first case, the runtime complexity here is big &lt;script type=&quot;math/tex&quot;&gt;O(n^{log_b a})&lt;/script&gt;, which is big &lt;script type=&quot;math/tex&quot;&gt;O(n^{log_2 7})&lt;/script&gt; or big &lt;script type=&quot;math/tex&quot;&gt;O(n^{2.81})&lt;/script&gt;. This beats the straightforward iterative approach &amp;amp; the regular block partitioning approach asymptotically!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We showed how Strassen’s algorithm was asymptotically faster than the basic procedure of multiplying matrices. Better asymptotic upper bounds for matrix multiplication have been found since Strassen’s algorithm came out in 1969. The most asymptotically efficient algorithm for multiplying n x n matrices to date is Coppersmith and Winograd’s algorithm, which has a running time of &lt;script type=&quot;math/tex&quot;&gt;O(n^{2.376})&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;However, in practice, Strassen’s algorithm is often not the method of choice for matrix multiplication. Cormen outlines the following four reasons for why:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;The constant factor hidden in the &lt;script type=&quot;math/tex&quot;&gt;O(n^{log_2 7})&lt;/script&gt; running time of Strassen’s algorithm is larger than the constant factor in the &amp;gt; &lt;script type=&quot;math/tex&quot;&gt;O(n^{3})&lt;/script&gt; SQUARE-MATRIX-MULTIPLY procedure.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;When the matrices are sparse, methods tailored for sparse matrices are faster.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;Strassen’s algorithm is not quite as numerically stable as the regular approach. In other words, because of the limited precision of computer arithmetic on noninteger values, larger errors accumulate in Strassen’s algorithm than in SQUARE-MATRIX-MULTIPLY.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;The submatrices formed at the levels of recursion consume space.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Cormen et al describe how the latter two reasons were mitigated around 1990. They state that the difference in numerical stability was overemphasized and although Strassen’s algorithm is too numerically unstable for some applications, it is within acceptable limits for others. In practice, fast matrix-multiplication implementations for dense matrices use Strassen’s algorithm for matrix sizes above a &lt;em&gt;crossover point&lt;/em&gt; and they switch to a simpler method once the subproblem size reduces to below the crossover point. The exact value of the crossover point is highly system dependent. Crossover points on various systems were found to range from 400 to 2150.&lt;/p&gt;

&lt;p&gt;In summary, even if algorithms like Strassen’s have lower asymptotic runtimes, they might not be implemented in practice due to issues with numerical stability. However, there can still be practical uses for them in certain scenarios, such as applying Strassen’s method when dealing with multiplications of large, dense matrices of a certain size.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Interactive visualizations using D3.js and D3 wrappers in Shiny</title><link href="http://localhost:4000/jekyll/update/2017/04/30/int-vis-d3-flights.html" rel="alternate" type="text/html" title="Interactive visualizations using D3.js and D3 wrappers in Shiny" /><published>2017-04-30T20:43:02-07:00</published><updated>2017-04-30T20:43:02-07:00</updated><id>http://localhost:4000/jekyll/update/2017/04/30/int-vis-d3-flights</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/04/30/int-vis-d3-flights.html">&lt;p&gt;I think D3.js is the one of the best libraries out there for interactive visualizations. This post will cover two cases; the first one uses D3.js while the second one uses D3 wrappers in Shiny.&lt;/p&gt;

&lt;h3 id=&quot;domestic-flights-by-city-in-the-us&quot;&gt;Domestic Flights by City in the US&lt;/h3&gt;

&lt;p&gt;The interactive plot is located here – &lt;a href=&quot;https://shivathudi.github.io/flights-chord/&quot;&gt;Chord Diagram&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The mouseover interactions can only be seen at the link above. The following screenshots only show some examples of static content and do not do justice to the actual figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/chord1.png&quot; alt=&quot;Chord1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/chord2.png&quot; alt=&quot;Chord2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The code for the chord diagram and the flights data can be found &lt;a href=&quot;https://shivathudi.github.io/flights-chord/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;diabetes-visualizations-in-shiny-using-d3-wrappers&quot;&gt;Diabetes visualizations in Shiny using D3 wrappers&lt;/h3&gt;

&lt;p&gt;Three different visualizations on a diabetes data set are located here – &lt;a href=&quot;https://shivathudi.shinyapps.io/diabetes-vis2/&quot;&gt;Diabetes Visualizations&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Again, to explore the full interactivity of these plots, I recommend that you visit the above link. The following examples are screenshots of static content.&lt;/p&gt;

&lt;h3 id=&quot;parallel-co-ordinates-plot-with-brushing&quot;&gt;Parallel Co-ordinates Plot with Brushing&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/diab1.png&quot; alt=&quot;diab1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, you can select certain ranges of variables using brushing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/diab1.png&quot; alt=&quot;diab2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scatterplot Matrix colored by different factors&lt;/p&gt;

&lt;p&gt;Here, there are three different factor variables that can be used to color the points – race, gender, and age.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/diab3.png&quot; alt=&quot;diab3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can select certain points in one square using brushing, and the same points are selected in the other squares:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/diab4.png&quot; alt=&quot;diab4&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;heatmaps-with-different-groupings&quot;&gt;Heatmaps with different groupings&lt;/h3&gt;

&lt;p&gt;For these heatmaps, a user can select groupings to visualize measures such as the time in the hospital, and the number of lab procedures performed on diabetes patients.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/diab5.png&quot; alt=&quot;diab5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The user can mouse over a particular cell to see the average value for that particular column or measure across the row or grouping.&lt;/p&gt;

&lt;p&gt;The code for all of the diabetes data visualizations can be found &lt;a href=&quot;https://github.com/shivathudi/data-visualization/tree/master/diabetes_data&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">I think D3.js is the one of the best libraries out there for interactive visualizations. This post will cover two cases; the first one uses D3.js while the second one uses D3 wrappers in Shiny.</summary></entry><entry><title type="html">Predicting Life Expectancy using Decision Tree Ensembles</title><link href="http://localhost:4000/jekyll/update/2017/04/12/life_exp.html" rel="alternate" type="text/html" title="Predicting Life Expectancy using Decision Tree Ensembles" /><published>2017-04-12T10:00:00-07:00</published><updated>2017-04-12T10:00:00-07:00</updated><id>http://localhost:4000/jekyll/update/2017/04/12/life_exp</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/04/12/life_exp.html">&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;

&lt;p&gt;In this post, I will predict life expectancy for the “average” person born in a certain year in one or more countries. I gathered all my data from Gapminder. For the predictor variables, I used birth rates, mortality rates (male, female, under5, and infant), HIV rates, GNI, and Internet usage rates. You can find this data in a compiled csv format (feature_set.csv) &lt;a href=&quot;https://github.com/shivathudi/machine-learning/blob/master/decision_trees/feature_set.csv&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There were some missing values of life expectancy in a few of the countries in the earlier years. I imputed these values using linear interpolation. I explored spline interpolation as well, but it did not improve the model’s performance in comparison with linear interpolation.&lt;/p&gt;

&lt;p&gt;I considered linear regression, decision trees, random forests, gradient boosting machines, and extra-trees regressors. I also built a validation set for predicting life expectancy for years outside the ranges of the training data provided, specifically for the years 1960 and 2011 to 2015.&lt;/p&gt;

&lt;p&gt;The training MSE and validation set MSE are below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/lifeexp_trees.png&quot; alt=&quot;illust1&quot; height=&quot;150px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The extra trees regressor performed better than random forests and gradient boosting machines, and the optimal number of trees for this algorithm was 40. As expected, a single decision tree overfits the training data and performs quite poorly on the validation set.&lt;/p&gt;

&lt;p&gt;Additional features that I would have liked to include were the total expenditure on health per capita, and pollution estimates. I was unable to acquire accurate estimates for these features, and data was mostly unavailable for the years we considered. However, I believe these features would help predict life expectancy more accurately.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;You can find my code &lt;a href=&quot;https://github.com/shivathudi/machine-learning/tree/master/decision_trees&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry></feed>